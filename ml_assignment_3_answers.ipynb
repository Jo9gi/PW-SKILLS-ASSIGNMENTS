{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jo9gi/PW-SKILLS-ASSIGNMENTS/blob/main/ml_assignment_4_answers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXxQmUbkNRV6"
      },
      "source": [
        "# Machine Learning Assignment 4 Answers\n",
        "\n",
        "This notebook contains comprehensive answers to the questions from the file 6650744c3afdd313879dc8b5.pdf."
      ],
      "id": "tXxQmUbkNRV6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QBOqwxiNRV8"
      },
      "source": [
        "### 1. What are ensemble techniques in machine learning?\n",
        "Ensemble techniques combine multiple models to improve predictive performance by reducing variance, bias, or improving predictions."
      ],
      "id": "7QBOqwxiNRV8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x4YDbjJNRV8"
      },
      "source": [
        "### 2. Explain bagging and how it works in ensemble techniques:\n",
        "Bagging (Bootstrap Aggregating) trains multiple models on different bootstrapped samples of the data and aggregates their predictions, typically by voting or averaging."
      ],
      "id": "9x4YDbjJNRV8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_9nQy6BNRV9"
      },
      "source": [
        "### 3. What is the purpose of bootstrapping in bagging?\n",
        "Bootstrapping creates diverse training sets by sampling with replacement, which helps reduce variance."
      ],
      "id": "o_9nQy6BNRV9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUayG-K1NRV9"
      },
      "source": [
        "### 4. Describe the random forest algorithm:\n",
        "Random forest builds multiple decision trees using bootstrapped data and random subsets of features, then aggregates their predictions."
      ],
      "id": "TUayG-K1NRV9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2mvL5dBNRV9"
      },
      "source": [
        "### 5. How does randomization reduce overfitting in random forests?\n",
        "Random feature selection and bootstrapping reduce correlation among trees, lowering overfitting."
      ],
      "id": "U2mvL5dBNRV9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmZoaciZNRV9"
      },
      "source": [
        "### 6. Explain the concept of feature bagging in random forests:\n",
        "Feature bagging randomly selects subsets of features for each tree split to increase diversity."
      ],
      "id": "zmZoaciZNRV9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qodxxbn0NRV-"
      },
      "source": [
        "### 7. What is the role of decision trees in gradient boosting?\n",
        "Decision trees are weak learners sequentially trained to correct errors of previous trees."
      ],
      "id": "Qodxxbn0NRV-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzvL-LYfNRV-"
      },
      "source": [
        "### 8. Differentiate between bagging and boosting:\n",
        "Bagging builds models independently; boosting builds models sequentially focusing on errors."
      ],
      "id": "PzvL-LYfNRV-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exEV045dNRV_"
      },
      "source": [
        "### 9. What is the AdaBoost algorithm, and how does it work?\n",
        "AdaBoost assigns weights to samples, trains weak learners sequentially, and adjusts weights to focus on misclassified samples."
      ],
      "id": "exEV045dNRV_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecI_y3N6NRV_"
      },
      "source": [
        "### 10. Explain the concept of weak learners in boosting algorithms:\n",
        "Weak learners are models slightly better than random guessing, combined to form a strong learner."
      ],
      "id": "ecI_y3N6NRV_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwcFCiq0NRV_"
      },
      "source": [
        "### 11. Describe the process of adaptive boosting:\n",
        "Sequentially trains weak learners, adjusting sample weights to emphasize difficult cases."
      ],
      "id": "cwcFCiq0NRV_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8opG0tnNRV_"
      },
      "source": [
        "### 12. How does AdaBoost adjust weights for misclassified data points?\n",
        "Increases weights of misclassified points to focus learning on them."
      ],
      "id": "K8opG0tnNRV_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urLyWea_NRWA"
      },
      "source": [
        "### 13. Discuss the XGBoost algorithm and its advantages over traditional gradient boosting:\n",
        "XGBoost is an optimized gradient boosting implementation with regularization, parallel processing, and handling missing data."
      ],
      "id": "urLyWea_NRWA"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTVCnqJ2NRWA"
      },
      "source": [
        "### 14. Explain the concept of regularization in XGBoost:\n",
        "Regularization penalizes model complexity to prevent overfitting."
      ],
      "id": "OTVCnqJ2NRWA"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6QKilwLNRWA"
      },
      "source": [
        "### 15. What are the different types of ensemble techniques?\n",
        "Bagging, boosting, stacking."
      ],
      "id": "b6QKilwLNRWA"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6Q4ExDtNRWA"
      },
      "source": [
        "### 16. Compare and contrast bagging and boosting:\n",
        "Bagging reduces variance by averaging; boosting reduces bias by sequential correction."
      ],
      "id": "j6Q4ExDtNRWA"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_FJtuUENRWA"
      },
      "source": [
        "### 17. Discuss the concept of ensemble diversity:\n",
        "Diverse models reduce correlated errors, improving ensemble performance."
      ],
      "id": "V_FJtuUENRWA"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4Rie58rNRWA"
      },
      "source": [
        "### 18. How do ensemble techniques improve predictive performance?\n",
        "By combining multiple models, they reduce errors and improve generalization."
      ],
      "id": "K4Rie58rNRWA"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqyEfV8RNRWA"
      },
      "source": [
        "### 19. Explain the concept of ensemble variance and bias:\n",
        "Variance is model sensitivity to data fluctuations; bias is error from incorrect assumptions."
      ],
      "id": "uqyEfV8RNRWA"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDtuzEFqNRWB"
      },
      "source": [
        "### 20. Discuss the trade-off between bias and variance in ensemble learning:\n",
        "Ensembles aim to reduce both bias and variance for better accuracy."
      ],
      "id": "bDtuzEFqNRWB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtwnFvAUNRWB"
      },
      "source": [
        "### 21. What are some common applications of ensemble techniques?\n",
        "Fraud detection, image classification, recommendation systems."
      ],
      "id": "BtwnFvAUNRWB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0zbF3WLNRWB"
      },
      "source": [
        "### 22. How does ensemble learning contribute to model interpretability?\n",
        "Some ensembles like random forests provide feature importance; others are less interpretable."
      ],
      "id": "l0zbF3WLNRWB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VICBJRinNRWB"
      },
      "source": [
        "### 23. Describe the process of stacking in ensemble learning:\n",
        "Combines predictions of base models using a meta-learner."
      ],
      "id": "VICBJRinNRWB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Y4dL22LNRWB"
      },
      "source": [
        "### 24. Discuss the role of meta-learners in stacking:\n",
        "Meta-learners learn to optimally combine base model outputs."
      ],
      "id": "1Y4dL22LNRWB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFdDeFRCNRWB"
      },
      "source": [
        "### 25. What are some challenges associated with ensemble techniques?\n",
        "Increased complexity, computational cost, reduced interpretability."
      ],
      "id": "iFdDeFRCNRWB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GlQUzcmNRWB"
      },
      "source": [
        "### 26. What is boosting, and how does it differ from bagging?\n",
        "Boosting builds models sequentially focusing on errors; bagging builds independently."
      ],
      "id": "2GlQUzcmNRWB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYSn3rkCNRWB"
      },
      "source": [
        "### 27. Explain the intuition behind boosting:\n",
        "Focus learning on hard-to-predict samples to improve accuracy."
      ],
      "id": "kYSn3rkCNRWB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDl9IUekNRWB"
      },
      "source": [
        "### 28. Describe the concept of sequential training in boosting:\n",
        "Models are trained one after another, each correcting previous errors."
      ],
      "id": "gDl9IUekNRWB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEoztF4uNRWB"
      },
      "source": [
        "### 29. How does boosting handle misclassified data points?\n",
        "By increasing their weights for subsequent learners."
      ],
      "id": "kEoztF4uNRWB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKN_5FqLNRWB"
      },
      "source": [
        "### 30. Discuss the role of weights in boosting algorithms:\n",
        "Weights emphasize difficult samples to guide learning."
      ],
      "id": "TKN_5FqLNRWB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mu1MMm8YNRWB"
      },
      "source": [
        "### 31. What is the difference between boosting and AdaBoost?\n",
        "AdaBoost is a specific boosting algorithm with adaptive weighting."
      ],
      "id": "Mu1MMm8YNRWB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scoTfRDVNRWC"
      },
      "source": [
        "### 32. How does AdaBoost adjust weights for misclassified samples?\n",
        "Increases weights exponentially for misclassified points."
      ],
      "id": "scoTfRDVNRWC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wb9adENoNRWC"
      },
      "source": [
        "### 33. Explain the concept of weak learners in boosting algorithms:\n",
        "Models with performance slightly better than random guessing."
      ],
      "id": "Wb9adENoNRWC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hn2yDcQqNRWC"
      },
      "source": [
        "### 34. Discuss the process of gradient boosting:\n",
        "Sequentially fits models to residual errors using gradient descent."
      ],
      "id": "Hn2yDcQqNRWC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TTMAVhmNRWC"
      },
      "source": [
        "### 35. What is the purpose of gradient descent in gradient boosting?\n",
        "To minimize loss by updating model parameters."
      ],
      "id": "6TTMAVhmNRWC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtEELZiUNRWC"
      },
      "source": [
        "### 36. Describe the role of learning rate in gradient boosting:\n",
        "Controls contribution of each tree to prevent overfitting."
      ],
      "id": "FtEELZiUNRWC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhp4q28dNRWC"
      },
      "source": [
        "### 37. How does gradient boosting handle overfitting?\n",
        "Using regularization, learning rate, and early stopping."
      ],
      "id": "jhp4q28dNRWC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFzJxs9HNRWC"
      },
      "source": [
        "### 38. Discuss the differences between gradient boosting and XGBoost:\n",
        "XGBoost adds regularization, parallelism, and system optimizations."
      ],
      "id": "VFzJxs9HNRWC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpbfl4RGNRWD"
      },
      "source": [
        "### 39. Explain the concept of regularized boosting:\n",
        "Boosting with penalties to reduce model complexity."
      ],
      "id": "dpbfl4RGNRWD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pUpuK6TNRWD"
      },
      "source": [
        "### 40. What are the advantages of using XGBoost over traditional gradient boosting?\n",
        "Faster training, better accuracy, handling missing data."
      ],
      "id": "9pUpuK6TNRWD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFlz6Gt3NRWD"
      },
      "source": [
        "### 41. Describe the process of early stopping in boosting algorithms:\n",
        "Stops training when validation error stops improving."
      ],
      "id": "eFlz6Gt3NRWD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZN9erZwNRWD"
      },
      "source": [
        "### 42. How does early stopping prevent overfitting?\n",
        "Prevents unnecessary training beyond optimal point."
      ],
      "id": "5ZN9erZwNRWD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLh4R7O8NRWD"
      },
      "source": [
        "### 43. Discuss the role of hyperparameters in boosting algorithms:\n",
        "Control model complexity and learning behavior."
      ],
      "id": "iLh4R7O8NRWD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owyvJQ2iNRWD"
      },
      "source": [
        "### 44. What are some common challenges associated with boosting?\n",
        "Overfitting, sensitivity to noisy data."
      ],
      "id": "owyvJQ2iNRWD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKx9XS3ONRWD"
      },
      "source": [
        "### 45. Explain the concept of boosting convergence:\n",
        "Model performance stabilizes as more learners are added."
      ],
      "id": "ZKx9XS3ONRWD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdlSx66XNRWD"
      },
      "source": [
        "### 46. How does boosting improve the performance of weak learners?\n",
        "By combining them to form a strong learner."
      ],
      "id": "OdlSx66XNRWD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGDpskdTNRWD"
      },
      "source": [
        "### 47. Discuss the impact of data imbalance on boosting algorithms:\n",
        "May bias towards majority class; requires handling."
      ],
      "id": "MGDpskdTNRWD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDERDA_ENRWD"
      },
      "source": [
        "### 48. What are some real-world applications of boosting?\n",
        "Credit scoring, customer churn prediction."
      ],
      "id": "CDERDA_ENRWD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Youi7pL6NRWD"
      },
      "source": [
        "### 49. Describe the process of ensemble selection in boosting:\n",
        "Choosing best subset of models for final prediction."
      ],
      "id": "Youi7pL6NRWD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OysFt_TCNRWE"
      },
      "source": [
        "### 50. How does boosting contribute to model interpretability?\n",
        "Provides feature importance but less transparent than single models."
      ],
      "id": "OysFt_TCNRWE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f_JaDlONRWE"
      },
      "source": [
        "### 51. Explain the curse of dimensionality and its impact on KNN:\n",
        "High dimensions cause distance measures to lose meaning, degrading KNN."
      ],
      "id": "2f_JaDlONRWE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1QZgZPFNRWE"
      },
      "source": [
        "### 52. What are the applications of KNN in real-world scenarios?\n",
        "Recommendation systems, image recognition."
      ],
      "id": "i1QZgZPFNRWE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWexSux9NRWE"
      },
      "source": [
        "### 53. Discuss the concept of weighted KNN:\n",
        "Weights neighbors by distance for prediction."
      ],
      "id": "VWexSux9NRWE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgFjVrp0NRWE"
      },
      "source": [
        "### 54. How do you handle missing values in KNN?\n",
        "Impute missing data or use distance metrics that handle missingness."
      ],
      "id": "UgFjVrp0NRWE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mb0SE7OuNRWE"
      },
      "source": [
        "### 55. Explain the difference between lazy learning and eager learning algorithms, and where does KNN fit in:\n",
        "KNN is lazy learning; it delays generalization until prediction."
      ],
      "id": "Mb0SE7OuNRWE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vqyz_8wNRWE"
      },
      "source": [
        "### 56. What are some methods to improve the performance of KNN?\n",
        "Feature scaling, dimensionality reduction."
      ],
      "id": "1vqyz_8wNRWE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3s3qDLYONRWF"
      },
      "source": [
        "### 57. Can KNN be used for regression tasks? If yes, how?\n",
        "Yes, by averaging target values of neighbors."
      ],
      "id": "3s3qDLYONRWF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7k2GiRQINRWF"
      },
      "source": [
        "### 58. Describe the boundary decision made by the KNN algorithm:\n",
        "Decision boundary is non-linear and depends on neighbors."
      ],
      "id": "7k2GiRQINRWF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUUx8WrMNRWF"
      },
      "source": [
        "### 59. How do you choose the optimal value of K in KNN?\n",
        "Using cross-validation or elbow method."
      ],
      "id": "iUUx8WrMNRWF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUELisSrNRWF"
      },
      "source": [
        "### 60. Discuss the trade-offs between using a small and large value of K in KNN:\n",
        "Small K is sensitive to noise; large K smooths but may miss patterns."
      ],
      "id": "YUELisSrNRWF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UezK8u_5NRWF"
      },
      "source": [
        "### 61. Explain the process of feature scaling in the context of KNN:\n",
        "Rescale features to prevent dominance by large-scale features."
      ],
      "id": "UezK8u_5NRWF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bd9ioSmQNRWF"
      },
      "source": [
        "### 62. Compare and contrast KNN with other classification algorithms like SVM and Decision Trees:\n",
        "KNN is instance-based and non-parametric; SVM finds optimal hyperplane; decision trees split data hierarchically."
      ],
      "id": "Bd9ioSmQNRWF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyST7_rXNRWF"
      },
      "source": [
        "### 63. How does the choice of distance metric affect the performance of KNN:\n",
        "Different metrics capture different notions of similarity."
      ],
      "id": "vyST7_rXNRWF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZ6NbQBeNRWF"
      },
      "source": [
        "### 64. What are some techniques to deal with imbalanced datasets in KNN:\n",
        "Resampling, weighted voting."
      ],
      "id": "fZ6NbQBeNRWF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBKeq22aNRWG"
      },
      "source": [
        "### 65. Explain the concept of cross-validation in the context of tuning KNN parameters:\n",
        "Partition data to evaluate model stability."
      ],
      "id": "gBKeq22aNRWG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9V4CL5xNRWG"
      },
      "source": [
        "### 66. What is the difference between uniform and distance-weighted voting in KNN:\n",
        "Uniform treats all neighbors equally; distance-weighted gives closer neighbors more influence."
      ],
      "id": "-9V4CL5xNRWG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQTMhFifNRWG"
      },
      "source": [
        "### 67. Discuss the computational complexity of KNN:\n",
        "High at prediction time due to distance calculations."
      ],
      "id": "eQTMhFifNRWG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lX3Zvkm4NRWG"
      },
      "source": [
        "### 68. How does the choice of distance metric impact the sensitivity of KNN to outliers:\n",
        "Some metrics are more robust to outliers."
      ],
      "id": "lX3Zvkm4NRWG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_c5LerdLNRWG"
      },
      "source": [
        "### 69. Explain the process of selecting an appropriate value for K using the elbow method:\n",
        "Plot error vs K and choose K at the elbow point."
      ],
      "id": "_c5LerdLNRWG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3TttCIINRWG"
      },
      "source": [
        "### 70. Can KNN be used for text classification tasks? If yes, how:\n",
        "Yes, using vectorized text features."
      ],
      "id": "r3TttCIINRWG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKtPOuPfNRWG"
      },
      "source": [
        "### 71. How do you decide the number of principal components to retain in PCA:\n",
        "Using explained variance threshold."
      ],
      "id": "XKtPOuPfNRWG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ss7KioSFNRWG"
      },
      "source": [
        "### 72. Explain the reconstruction error in the context of PCA:\n",
        "Difference between original and reconstructed data."
      ],
      "id": "Ss7KioSFNRWG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvAcW9WqNRWG"
      },
      "source": [
        "### 73. What are the applications of PCA in real-world scenarios:\n",
        "Image compression, noise reduction."
      ],
      "id": "IvAcW9WqNRWG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psZEErYXNRWG"
      },
      "source": [
        "### 74. Discuss the limitations of PCA:\n",
        "Linear method, sensitive to outliers."
      ],
      "id": "psZEErYXNRWG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "972rqHWANRWH"
      },
      "source": [
        "### 75. What is Singular Value Decomposition (SVD), and how is it related to PCA:\n",
        "SVD is a matrix factorization used to compute PCA."
      ],
      "id": "972rqHWANRWH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7u6ZV3LnNRWH"
      },
      "source": [
        "### 76. Explain the concept of latent semantic analysis (LSA) and its application in natural language processing:\n",
        "LSA uses SVD to extract topics from text."
      ],
      "id": "7u6ZV3LnNRWH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5rAGFTCNRWH"
      },
      "source": [
        "### 77. What are some alternatives to PCA for dimensionality reduction:\n",
        "t-SNE, ICA, autoencoders."
      ],
      "id": "k5rAGFTCNRWH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2egtCFlNRWH"
      },
      "source": [
        "### 78. Describe t-distributed Stochastic Neighbor Embedding (t-SNE) and its advantages over PCA:\n",
        "Non-linear method preserving local structure."
      ],
      "id": "r2egtCFlNRWH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYAwqyvaNRWH"
      },
      "source": [
        "### 79. How does t-SNE preserve local structure compared to PCA:\n",
        "By modeling pairwise similarities."
      ],
      "id": "dYAwqyvaNRWH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pc6ezUYBNRWH"
      },
      "source": [
        "### 80. Discuss the limitations of t-SNE:\n",
        "Computationally expensive, non-parametric."
      ],
      "id": "Pc6ezUYBNRWH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWnBP6dDNRWH"
      },
      "source": [
        "### 81. What is the difference between PCA and Independent Component Analysis (ICA):\n",
        "PCA finds uncorrelated components; ICA finds statistically independent components."
      ],
      "id": "AWnBP6dDNRWH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q03Kk5xaNRWH"
      },
      "source": [
        "### 82. Explain the concept of manifold learning and its significance in dimensionality reduction:\n",
        "Non-linear dimensionality reduction preserving data geometry."
      ],
      "id": "q03Kk5xaNRWH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlaMlGv_NRWH"
      },
      "source": [
        "### 83. What are autoencoders, and how are they used for dimensionality reduction:\n",
        "Neural networks that learn compressed representations."
      ],
      "id": "KlaMlGv_NRWH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttySSw1XNRWH"
      },
      "source": [
        "### 84. Discuss the challenges of using nonlinear dimensionality reduction techniques:\n",
        "Computational cost, interpretability."
      ],
      "id": "ttySSw1XNRWH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDt1Nq5dNRWH"
      },
      "source": [
        "### 85. How does the choice of distance metric impact the performance of dimensionality reduction techniques:\n",
        "Affects neighborhood preservation."
      ],
      "id": "RDt1Nq5dNRWH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmHnH0eNNRWI"
      },
      "source": [
        "### 86. What are some techniques to visualize high-dimensional data after dimensionality reduction:\n",
        "Scatter plots, heatmaps."
      ],
      "id": "hmHnH0eNNRWI"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22ZcQXhsNRWI"
      },
      "source": [
        "### 87. Explain the concept of feature hashing and its role in dimensionality reduction:\n",
        "Hashing features to reduce dimensionality efficiently."
      ],
      "id": "22ZcQXhsNRWI"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5CQeZwWNRWI"
      },
      "source": [
        "### 88. What is the difference between global and local feature extraction methods:\n",
        "Global methods consider entire data; local focus on neighborhoods."
      ],
      "id": "x5CQeZwWNRWI"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmliaZdqNRWI"
      },
      "source": [
        "### 89. How does feature sparsity affect the performance of dimensionality reduction techniques:\n",
        "Sparse data can degrade performance."
      ],
      "id": "EmliaZdqNRWI"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2WM98cGNRWI"
      },
      "source": [
        "### 90. Discuss the impact of outliers on dimensionality reduction algorithms:\n",
        "Outliers can distort components and embeddings."
      ],
      "id": "r2WM98cGNRWI"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
